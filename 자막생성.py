import os
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, scrolledtext
import threading
import gc
import whisper
from transformers import pipeline
import torch

class SubtitleTranslatorApp:
    def __init__(self, root):
        self.root = root
        self.root.title("AI ëª¨ë¸ ê¸°ë°˜ ìë§‰ìƒì„±")
        self.root.geometry("400x850")

        self.whisper_model = None
        self.nllb_model = None

        self.gpuinfo_whisper = {
            "tiny": "CPU, GTX1050 2GB~",
            "base": "CPU, GTX1050 2GB~",
            "small": "GTX1650 4GB~",
            "medium": "RTX3060 6GB~",
            "large": "RTX3060 12GB~",
            "turbo": "RTX3060 12GB~",
            "large-v2": "RTX3060 12GB~",
            "large-v3": "RTX3060 12GB~",
        }
        self.gpuinfo_nllb = {
            "NLLB-200-600M": "CPU, GTX1050 2GB~",
            "NLLB-200-1.3B": "GTX1650 4GB~",
            "NLLB-200-3.3B": "RTX3060 12GB~",
        }

        self.language_codes = {
            "ì˜ì–´": "eng_Latn",
            "í•œêµ­ì–´": "kor_Hang",
            "ì¼ë³¸ì–´": "jpn_Jpan",
            "ì¤‘êµ­ì–´(ê°„ì²´)": "zho_Hans",
            "ìŠ¤í˜ì¸ì–´": "spa_Latn"
        }

        self.init_ui()

    def init_ui(self):
        main_frame = ttk.Frame(self.root, padding=10)
        main_frame.pack(fill=tk.BOTH, expand=True)

        # íŒŒì¼ ê´€ë¦¬
        file_frame = ttk.LabelFrame(main_frame, text="íŒŒì¼ ê´€ë¦¬", padding=10)
        file_frame.pack(fill=tk.X, pady=5)
        ttk.Button(file_frame, text="ë¹„ë””ì˜¤ ì„ íƒ", command=self.select_files).pack(side=tk.LEFT)
        self.file_list = tk.Listbox(file_frame, height=8)
        self.file_list.pack(fill=tk.X, pady=5)

        # ëª¨ë¸ ì„¤ì •
        model_frame = ttk.LabelFrame(main_frame, text="ëª¨ë¸ ì„¤ì •", padding=10)
        model_frame.pack(fill=tk.X, pady=5)
        
        ttk.Label(model_frame, text="Whisper ëª¨ë¸:").grid(row=0, column=0, sticky=tk.W)
        self.whisper_model_var = ttk.Combobox(
            model_frame,
            values=['tiny', 'base', 'small', 'medium', 'large', 'turbo', 'large-v2', 'large-v3']
        )
        self.whisper_model_var.set('medium')
        self.whisper_model_var.grid(row=0, column=1)

        ttk.Label(model_frame, text="NLLB200 ëª¨ë¸:").grid(row=1, column=0, sticky=tk.W)
        self.nllb_model_var = ttk.Combobox(
            model_frame,
            values=['NLLB-200-600M', 'NLLB-200-1.3B', 'NLLB-200-3.3B']
        )
        self.nllb_model_var.set('NLLB-200-1.3B')
        self.nllb_model_var.grid(row=1, column=1)

        ttk.Label(model_frame, text="ì—°ì‚° ë°©ì‹:").grid(row=2, column=0, sticky=tk.W)
        self.device_var = ttk.Combobox(
            model_frame,
            values=['cpu', 'gpu']
        )
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ ê¸°ë³¸ê°’ 'gpu' ì„¤ì •
        self.device_var.set('gpu' if torch.cuda.is_available() else 'cpu')
        self.device_var.grid(row=2, column=1)

        # ì¶”ì²œ GPU ì •ë³´
        self.gpu_label_whisper = ttk.Label(model_frame, text="Whisper ì¶”ì²œ: ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.")
        self.gpu_label_whisper.grid(row=3, column=0, columnspan=2, pady=2)
        self.gpu_label_nllb = ttk.Label(model_frame, text="NLLB200 ì¶”ì²œ: ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.")
        self.gpu_label_nllb.grid(row=4, column=0, columnspan=2, pady=2)

        self.whisper_model_var.bind("<<ComboboxSelected>>", self.update_gpuinfo)
        self.nllb_model_var.bind("<<ComboboxSelected>>", self.update_gpuinfo)

        # ì–¸ì–´ ì„¤ì •
        lang_frame = ttk.LabelFrame(main_frame, text="ì–¸ì–´ ì„¤ì •", padding=10)
        lang_frame.pack(fill=tk.X, pady=5)
        ttk.Label(lang_frame, text="ì…ë ¥ ì–¸ì–´:").grid(row=0, column=0)
        self.src_lang = ttk.Combobox(lang_frame, values=list(self.language_codes.keys()))
        self.src_lang.set("ì˜ì–´")
        self.src_lang.grid(row=0, column=1)
        ttk.Label(lang_frame, text="ì¶œë ¥ ì–¸ì–´:").grid(row=1, column=0)
        self.tgt_lang = ttk.Combobox(lang_frame, values=list(self.language_codes.keys()))
        self.tgt_lang.set("í•œêµ­ì–´")
        self.tgt_lang.grid(row=1, column=1)

        # ì§„í–‰ í‘œì‹œì¤„
        self.progress = ttk.Progressbar(main_frame, mode='indeterminate')
        self.progress.pack(fill=tk.X, pady=5)

        # ë¡œê·¸ ì˜ì—­
        self.log_area = scrolledtext.ScrolledText(main_frame, height=15)
        self.log_area.pack(fill=tk.BOTH, expand=True, pady=5)

        # ì‹œì‘ ë²„íŠ¼
        ttk.Button(main_frame, text="ì²˜ë¦¬ ì‹œì‘", command=self.start_processing).pack(pady=10)

        self.update_gpuinfo() # ì´ˆê¸° GPU ì •ë³´ ì—…ë°ì´íŠ¸

    def update_gpuinfo(self, event=None):
        whisper_name = self.whisper_model_var.get()
        nllb_name = self.nllb_model_var.get()
        info_w = self.gpuinfo_whisper.get(whisper_name, "ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.")
        info_n = self.gpuinfo_nllb.get(nllb_name, "ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”.")
        self.gpu_label_whisper.config(text=f"Whisper ì¶”ì²œ: {info_w}")
        self.gpu_label_nllb.config(text=f"NLLB200 ì¶”ì²œ: {info_n}")

    def select_files(self):
        files = filedialog.askopenfilenames(
            title='ë¹„ë””ì˜¤ íŒŒì¼ ì„ íƒ',
            filetypes=(('ë¹„ë””ì˜¤ íŒŒì¼', '*.mp4 *.avi *.mkv *.mov'), ('ëª¨ë“  íŒŒì¼', '*.*'))
        )
        self.file_list.delete(0, tk.END)
        for file in files:
            self.file_list.insert(tk.END, file)

    def log(self, message):
        self.log_area.insert(tk.END, f"{message}\n")
        self.log_area.see(tk.END)
        self.root.update_idletasks()

    def generate_subtitles(self, file_path):
        try:
            base_name = os.path.splitext(file_path)[0]
            srt_path = f"{base_name}.srt"
            self.log(f"ìë§‰ ìƒì„± ì¤‘: {os.path.basename(file_path)}")
            
            # Whisperì˜ transcribe í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìë§‰ ìƒì„±
            result = self.whisper_model.transcribe(file_path)
            
            # SRT íŒŒì¼ë¡œ ì €ì¥
            writer = whisper.utils.get_writer("srt", os.path.dirname(file_path))
            writer(result, os.path.basename(srt_path))
            return srt_path
        except Exception as e:
            self.log(f"ìë§‰ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None

    def translate_subtitle(self, srt_path, src_code, tgt_code):
        try:
            self.log(f"ë²ˆì—­ ì‹œì‘: {os.path.basename(srt_path)}")
            
            with open(srt_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            translated_content = ""
            blocks = content.strip().split('\n\n')
            
            for block in blocks:
                lines = block.strip().split('\n')
                
                # ìë§‰ ë¸”ë¡ì´ ìœ íš¨í•œì§€ í™•ì¸ (ë²ˆí˜¸, ì‹œê°„, í…ìŠ¤íŠ¸)
                if len(lines) < 3:
                    translated_content += block + '\n\n'
                    continue
                    
                header = '\n'.join(lines[:2]) # ë²ˆí˜¸ì™€ ì‹œê°„ ì •ë³´
                text = '\n'.join(lines[2:]) # ì‹¤ì œ ìë§‰ í…ìŠ¤íŠ¸
                
                if text.strip():
                    # NLLB ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë²ˆì—­
                    translated = self.nllb_model(
                        text,
                        src_lang=src_code,
                        tgt_lang=tgt_code,
                        max_length=512
                    )[0]['translation_text']
                else:
                    translated = "" # í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ìœ ì§€
                    
                translated_content += f"{header}\n{translated}\n\n"
                
            translated_path = f"{os.path.splitext(srt_path)[0]}_translated.srt"
            
            with open(translated_path, 'w', encoding='utf-8') as f:
                f.write(translated_content)
                
            return True
            
        except Exception as e:
            self.log(f"ë²ˆì—­ ì˜¤ë¥˜: {str(e)}")
            return False

    def start_processing(self):
        """ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•˜ëŠ” ìŠ¤ë ˆë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        files = [self.file_list.get(i) for i in range(self.file_list.size())]
        if not files:
            messagebox.showwarning("ê²½ê³ ", "íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”")
            return
        # ë©”ì¸ UIë¥¼ ë©ˆì¶”ì§€ ì•Šë„ë¡ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
        threading.Thread(target=self.process_pipeline, args=(files,), daemon=True).start()

    def cleanup(self, device_mode):
        if self.whisper_model:
            del self.whisper_model
            self.whisper_model = None
        if self.nllb_model:
            del self.nllb_model
            self.nllb_model = None
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        gc.collect()
        
        # GPU ëª¨ë“œì¸ ê²½ìš° CUDA ë©”ëª¨ë¦¬ ìºì‹œ ë¹„ìš°ê¸°
        if device_mode == "gpu" and torch.cuda.is_available():
            torch.cuda.empty_cache()

    def process_pipeline(self, files):
        self.progress.start()
        device_mode = self.device_var.get()
        actual_device = device_mode
        
        try:
            # 1. ì‹¤ì œ ì—°ì‚° ì¥ì¹˜ ì„¤ì • ë° í™•ì¸
            if device_mode == "gpu" and not torch.cuda.is_available():
                actual_device = "cpu"
                self.log("ê²½ê³ : GPU ëª¨ë“œë¡œ ì„¤ì •ë˜ì—ˆìœ¼ë‚˜ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            
            # --- 2. WHISPER ëª¨ë¸ ë¡œë“œ (ìë§‰ ìƒì„±) ---
            model_name = self.whisper_model_var.get()
            self.log(f"Whisper ëª¨ë¸ ë¡œë“œ ì¤‘... ({model_name}, device: {actual_device})")
            
            # **ëª¨ë¸ ë¡œë“œ ì•ˆì •í™” ë¡œì§ ì ìš© (UntypedStorage ì˜¤ë¥˜ ë°©ì§€)**
            if actual_device == "gpu":
                 # 'cuda' ëŒ€ì‹  ëª…ì‹œì ì¸ device=Noneì„ ì‚¬ìš©í•˜ê±°ë‚˜, 
                 # ì•ˆì •ì ì¸ ì¥ì¹˜ ì´ë¦„ 'cuda'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
                 self.whisper_model = whisper.load_model(model_name, device="cuda")
            else:
                 self.whisper_model = whisper.load_model(model_name, device="cpu")

            # ìë§‰ ìƒì„±
            subtitle_paths = []
            for file in files:
                srt_path = self.generate_subtitles(file)
                if srt_path:
                    subtitle_paths.append(srt_path)
                    
            # --- 3. WHISPER ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ í™•ë³´) ---
            self.log("Whisper ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
            self.cleanup(device_mode) # cleanup í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ë©”ëª¨ë¦¬ ì •ë¦¬

            # --- 4. NLLB ëª¨ë¸ ë¡œë“œ (ë²ˆì—­) ---
            src_lang_name = self.src_lang.get()
            tgt_lang_name = self.tgt_lang.get()

            if src_lang_name == tgt_lang_name:
                # ì…ë ¥ ì–¸ì–´ì™€ ì¶œë ¥ ì–¸ì–´ê°€ ê°™ìœ¼ë©´ ë²ˆì—­ ê±´ë„ˆë›°ê¸°
                self.log("ì…ë ¥ ì–¸ì–´ì™€ ì¶œë ¥ ì–¸ì–´ê°€ ê°™ì•„ ë²ˆì—­ ë‹¨ê³„ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            else:
                nllb_name = self.nllb_model_var.get()
                self.log(f"NLLB200 ëª¨ë¸ ë¡œë“œ ì¤‘... ({nllb_name}, device: {device_mode})")
                
                if nllb_name == "NLLB-200-600M":
                    model_path = "facebook/nllb-200-distilled-600M"
                elif nllb_name == "NLLB-200-1.3B":
                    model_path = "facebook/nllb-200-distilled-1.3B"
                else:
                    model_path = "facebook/nllb-200-3.3B"

                # **NLLB ëª¨ë¸ ë¡œë“œ ì‹œ ì¥ì¹˜ ì„¤ì • ê°œì„ **
                if device_mode == "gpu" and torch.cuda.is_available():
                    # device=0ì€ ì²« ë²ˆì§¸ GPUë¥¼ ì˜ë¯¸ (CUDA ì¸ë±ìŠ¤)
                    device_param = 0 
                else:
                    # device=-1ì€ CPUë¥¼ ì˜ë¯¸
                    device_param = -1
                
                self.nllb_model = pipeline(
                    'translation',
                    model=model_path,
                    device=device_param
                )

                # ë²ˆì—­ ì‹¤í–‰
                src_code = self.language_codes[src_lang_name]
                tgt_code = self.language_codes[tgt_lang_name]
                
                for srt_path in subtitle_paths:
                    if self.translate_subtitle(srt_path, src_code, tgt_code):
                        self.log(f"ë²ˆì—­ ì™„ë£Œ: {os.path.basename(srt_path).replace('.srt', '_translated.srt')}")
                        
            # --- 5. NLLB ëª¨ë¸ ì–¸ë¡œë“œ ---
            self.log("NLLB200 ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
            self.cleanup(device_mode)
            
            self.log("âœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            self.log(f"ğŸš¨ ì „ì²´ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            self.cleanup(device_mode) # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì •ë¦¬
            
        finally:
            self.progress.stop()

if __name__ == "__main__":
    # CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ë¥¼ ë¨¼ì € í™•ì¸í•˜ê³  PyTorch ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ë¥¼ ì ê²€í•©ë‹ˆë‹¤.
    if not (torch.cuda.is_available() or torch.has_mps):
        print("ê²½ê³ : CUDA ë˜ëŠ” MPSë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ì²˜ë¦¬ëŠ” CPU ëª¨ë“œë¡œ ì§„í–‰ë©ë‹ˆë‹¤.")
    
    root = tk.Tk()
    app = SubtitleTranslatorApp(root)
    root.mainloop()

